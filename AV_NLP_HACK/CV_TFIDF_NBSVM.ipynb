{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data = pd.read_csv(\"data/Train.csv\")\n",
    "tags = pd.read_csv(\"data/Tags.csv\")\n",
    "test_data = pd.read_csv(\"data/Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14004 entries, 0 to 14003\n",
      "Data columns (total 31 columns):\n",
      " #   Column                                        Non-Null Count  Dtype \n",
      "---  ------                                        --------------  ----- \n",
      " 0   id                                            14004 non-null  int64 \n",
      " 1   ABSTRACT                                      14004 non-null  object\n",
      " 2   Computer Science                              14004 non-null  int64 \n",
      " 3   Mathematics                                   14004 non-null  int64 \n",
      " 4   Physics                                       14004 non-null  int64 \n",
      " 5   Statistics                                    14004 non-null  int64 \n",
      " 6   Analysis of PDEs                              14004 non-null  int64 \n",
      " 7   Applications                                  14004 non-null  int64 \n",
      " 8   Artificial Intelligence                       14004 non-null  int64 \n",
      " 9   Astrophysics of Galaxies                      14004 non-null  int64 \n",
      " 10  Computation and Language                      14004 non-null  int64 \n",
      " 11  Computer Vision and Pattern Recognition       14004 non-null  int64 \n",
      " 12  Cosmology and Nongalactic Astrophysics        14004 non-null  int64 \n",
      " 13  Data Structures and Algorithms                14004 non-null  int64 \n",
      " 14  Differential Geometry                         14004 non-null  int64 \n",
      " 15  Earth and Planetary Astrophysics              14004 non-null  int64 \n",
      " 16  Fluid Dynamics                                14004 non-null  int64 \n",
      " 17  Information Theory                            14004 non-null  int64 \n",
      " 18  Instrumentation and Methods for Astrophysics  14004 non-null  int64 \n",
      " 19  Machine Learning                              14004 non-null  int64 \n",
      " 20  Materials Science                             14004 non-null  int64 \n",
      " 21  Methodology                                   14004 non-null  int64 \n",
      " 22  Number Theory                                 14004 non-null  int64 \n",
      " 23  Optimization and Control                      14004 non-null  int64 \n",
      " 24  Representation Theory                         14004 non-null  int64 \n",
      " 25  Robotics                                      14004 non-null  int64 \n",
      " 26  Social and Information Networks               14004 non-null  int64 \n",
      " 27  Statistics Theory                             14004 non-null  int64 \n",
      " 28  Strongly Correlated Electrons                 14004 non-null  int64 \n",
      " 29  Superconductivity                             14004 non-null  int64 \n",
      " 30  Systems and Control                           14004 non-null  int64 \n",
      "dtypes: int64(30), object(1)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_COLS = ['Computer Science','Mathematics','Physics','Statistics']\n",
    "TAGS = list(tags['Tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a ever-growing datasets inside observational astronomy have challenged scientists inside many aspects, including an efficient and interactive data exploration and visualization. many tools have been developed to confront this challenge. however, they usually focus on displaying a actual images or focus on visualizing patterns within catalogs inside the predefined way. inside this paper we introduce vizic, the python visualization library that builds a connection between images and catalogs through an interactive map of a sky region. vizic visualizes catalog data over the custom background canvas with the help of a shape, size and orientation of each object inside a catalog. a displayed objects inside a map are highly interactive and customizable comparing to those inside a images. these objects should be filtered by or colored by their properties, such as redshift and magnitude. they also should be sub-selected with the help of the lasso-like tool considering further analysis with the help of standard python functions from in the jupyter notebook. furthermore, vizic allows custom overlays to be appended dynamically on top of a sky map. we have initially implemented several overlays, namely, voronoi, delaunay, minimum spanning tree and healpix grid layers, which are helpful considering visualizing large-scale structure. all these overlays should be generated, added or removed interactively with one line of code. a catalog data was stored inside the non-relational database, and a interfaces were developed inside javascript and python to work within jupyter notebook, which allows to create custom widgets, user generated scripts to analyze and plot a data selected/displayed inside a interactive map. this unique design makes vizic the very powerful and flexible interactive analysis tool. vizic should be adopted inside variety of exercises, considering example, data inspection, clustering analysis, galaxy alignment studies, outlier identification or simply large-scale visualizations.',\n",
       " \"we propose the framework considering optimal $t$-matchings excluding a prescribed $t$-factors inside bipartite graphs. a proposed framework was the generalization of a nonbipartite matching problem and includes several problems, such as a triangle-free $2$-matching, square-free $2$-matching, even factor, and arborescence problems. inside this paper, we demonstrate the unified understanding of these problems by commonly extending previous important results. we solve our problem under the reasonable assumption, which was sufficiently broad to include a specific problems listed above. we first present the min-max theorem and the combinatorial algorithm considering a unweighted version. we then provide the linear programming formulation with dual integrality and the primal-dual algorithm considering a weighted version. the key ingredient of a proposed algorithm was the technique to shrink forbidden structures, which corresponds to a techniques of shrinking odd cycles, triangles, squares, and directed cycles inside edmonds' blossom algorithm, the triangle-free $2$-matching algorithm, the square-free $2$-matching algorithm, and an arborescence algorithm, respectively.\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_data['ABSTRACT'])[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['ABSTRACT'] = train_data['ABSTRACT'].str.lower()\n",
    "test_data['ABSTRACT'] = test_data['ABSTRACT'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "train_data['ABSTRACT'] = train_data['ABSTRACT'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "test_data['ABSTRACT'] = test_data['ABSTRACT'].str.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, val = train_test_split(train_data,test_size=0.2,random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Get best threshold for each label\n",
    "def get_cut_offthreshold(y_pred_prob,validation_set,TAGS):\n",
    "    '''\n",
    "    Get best threshold cut off for different labels to maximize the micro F1 score\n",
    "    '''\n",
    "    thresholds = np.array(list(range(0,100)))/100.0\n",
    "    best_thresholds = []\n",
    "    for idx in range(0,25):\n",
    "        scores = [f1_score(validation_set[TAGS[idx]], y_pred_prob[:,idx] > thresh, average='micro') for thresh in thresholds]\n",
    "        best_thresh = thresholds[np.argmax(scores)]\n",
    "        best_thresholds.append(best_thresh)\n",
    "    return best_thresholds\n",
    "\n",
    "def get_predictions(pred_prob,best_thresholds,TAGS):\n",
    "    '''\n",
    "    Get predictions based on probabilities and class specific thresholds\n",
    "    '''\n",
    "    predictions = np.zeros((pred_prob.shape[0],len(TAGS)))\n",
    "    for idx in range(0,25):\n",
    "        predictions[:,idx] = pred_prob[:,idx] > best_thresholds[idx]    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1:  Using Count Vectorizer + OneVsRestClassifier + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=10, max_iter=1000, n_jobs=1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Count Vectorizer + OneVsRestClassifier + Logistic Regression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "vec = CountVectorizer(max_features=10000)\n",
    "vec.fit(list(train['ABSTRACT']))\n",
    "        \n",
    "\n",
    "\n",
    "trn_abs = vec.transform(train['ABSTRACT'])\n",
    "val_abs = vec.transform(val['ABSTRACT'])\n",
    "test_abs = vec.transform(test_data['ABSTRACT'])\n",
    "\n",
    "clf = OneVsRestClassifier(LogisticRegression(C=10,n_jobs=1,max_iter=1000))\n",
    "clf.fit(trn_abs,train[TAGS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on Validation Set 0.6238731737643767\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = clf.predict_proba(val_abs)\n",
    "best_thresholds = get_cut_offthreshold(y_pred_prob,val,TAGS)\n",
    "y_pred = get_predictions(y_pred_prob,best_thresholds,TAGS)\n",
    "\n",
    "print(\"F1 Score on Validation Set\", f1_score(val[TAGS], y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on Test\n",
    "\n",
    "y_pred_test_prob = clf.predict_proba(test_abs)\n",
    "\n",
    "predictions = get_predictions(y_pred_test_prob,best_thresholds,TAGS)\n",
    "\n",
    "result = pd.DataFrame(predictions)\n",
    "result.columns = TAGS\n",
    "result['id'] = test_data['id']\n",
    "result.to_csv(\"cv_logistic.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2:  Using TFIDF Vectorizer + OneVsRestClassifier + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=10, max_iter=1000, n_jobs=1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "vec = TfidfVectorizer(max_features=10000)\n",
    "vec.fit(list(train['ABSTRACT']))\n",
    "\n",
    "\n",
    "TAGS = list(tags['Tags'])\n",
    "trn_abs = vec.transform(train['ABSTRACT'])\n",
    "val_abs = vec.transform(val['ABSTRACT'])\n",
    "test_abs = vec.transform(test_data['ABSTRACT'])\n",
    "\n",
    "clf = OneVsRestClassifier(LogisticRegression(C=10,n_jobs=1,max_iter=1000))\n",
    "clf.fit(trn_abs,train[TAGS])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on Validation Set 0.6869462492616657\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = clf.predict_proba(val_abs)\n",
    "best_thresholds = get_cut_offthreshold(y_pred_prob,val,TAGS)\n",
    "y_pred = get_predictions(y_pred_prob,best_thresholds,TAGS)\n",
    "\n",
    "print(\"F1 Score on Validation Set\", f1_score(val[TAGS], y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on Test\n",
    "y_pred_test_prob = clf.predict_proba(test_abs)\n",
    "\n",
    "predictions = get_predictions(y_pred_test_prob,best_thresholds,TAGS)\n",
    "\n",
    "result = pd.DataFrame(predictions)\n",
    "result.columns = TAGS\n",
    "result['id'] = test_data['id']\n",
    "result.to_csv(\"tfidf_logistic.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Using TFIDF Vectorizer With Topic Columns + OneVsRestClassifier + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "vec = TfidfVectorizer(max_features=10000)\n",
    "vec.fit(list(train['ABSTRACT']))\n",
    "        \n",
    "\n",
    "TAGS = list(tags['Tags'])\n",
    "trn_abs = vec.transform(train['ABSTRACT'])\n",
    "val_abs = vec.transform(val['ABSTRACT'])\n",
    "test_abs = vec.transform(test_data['ABSTRACT'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_abs = np.hstack((trn_abs.toarray(),train[TOPIC_COLS]))\n",
    "val_abs = np.hstack((val_abs.toarray(),val[TOPIC_COLS]))\n",
    "test_abs = np.hstack((test_abs.toarray(),test_data[TOPIC_COLS]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=10, max_iter=1000, n_jobs=1))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = OneVsRestClassifier(LogisticRegression(C=10,n_jobs=1,max_iter=1000))\n",
    "clf.fit(trn_abs,train[TAGS])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on Validation Set 0.7622750179985601\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = clf.predict_proba(val_abs)\n",
    "best_thresholds = get_cut_offthreshold(y_pred_prob,val,TAGS)\n",
    "y_pred = get_predictions(y_pred_prob,best_thresholds,TAGS)\n",
    "\n",
    "print(\"F1 Score on Validation Set\", f1_score(val[TAGS], y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on Test\n",
    "y_pred_test_prob = clf.predict_proba(test_abs)\n",
    "\n",
    "predictions = get_predictions(y_pred_test_prob,best_thresholds,TAGS)\n",
    "\n",
    "result = pd.DataFrame(predictions)\n",
    "result.columns = TAGS\n",
    "result['id'] = test_data['id']\n",
    "result.to_csv(\"tfidf_logistic_topic_cols.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4 : NB-SVM using TFIDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilities\n",
    "def get_prob(x_feats,y_i, y):\n",
    "    p = x_feats[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "# NB Log count ratios\n",
    "def get_nb_feats(x_feats,y):\n",
    "    log_rat = np.log(get_prob(x_feats,1,y) / get_prob(x_feats,0,y))\n",
    "    x_feats_nb = x_feats.multiply(log_rat)\n",
    "    return x_feats_nb,log_rat\n",
    "\n",
    "def get_model(x_feats,y):\n",
    "    y = y.values       \n",
    "    x_feats_nb,log_rat = get_nb_feats(x_feats,y)\n",
    "    model = LogisticRegression(C=10,max_iter=1000)\n",
    "    model.fit(x_feats_nb, y)\n",
    "    return model, log_rat\n",
    "\n",
    "def get_model_topic(x_feats,topic_cols,y):\n",
    "    y = y.values       \n",
    "    x_feats_nb,log_rat = get_nb_feats(x_feats,y)\n",
    "    x_feats_nb_topic_cols = np.hstack((x_feats_nb.toarray(),topic_cols))\n",
    "    model = LogisticRegression(C=10,max_iter=1000)\n",
    "    model.fit(x_feats_nb_topic_cols, y)\n",
    "    return model, log_rat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting .. Analysis of PDEs\n",
      "Fitting .. Applications\n",
      "Fitting .. Artificial Intelligence\n",
      "Fitting .. Astrophysics of Galaxies\n",
      "Fitting .. Computation and Language\n",
      "Fitting .. Computer Vision and Pattern Recognition\n",
      "Fitting .. Cosmology and Nongalactic Astrophysics\n",
      "Fitting .. Data Structures and Algorithms\n",
      "Fitting .. Differential Geometry\n",
      "Fitting .. Earth and Planetary Astrophysics\n",
      "Fitting .. Fluid Dynamics\n",
      "Fitting .. Information Theory\n",
      "Fitting .. Instrumentation and Methods for Astrophysics\n",
      "Fitting .. Machine Learning\n",
      "Fitting .. Materials Science\n",
      "Fitting .. Methodology\n",
      "Fitting .. Number Theory\n",
      "Fitting .. Optimization and Control\n",
      "Fitting .. Representation Theory\n",
      "Fitting .. Robotics\n",
      "Fitting .. Social and Information Networks\n",
      "Fitting .. Statistics Theory\n",
      "Fitting .. Strongly Correlated Electrons\n",
      "Fitting .. Superconductivity\n",
      "Fitting .. Systems and Control\n",
      "F1 Score on Validation Set 0.679727180380206\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "vec = TfidfVectorizer(ngram_range=(1,2), min_df=3, max_df=0.9,use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1,max_features=10000)\n",
    "\n",
    "trn_term_doc = vec.fit_transform(train['ABSTRACT'])\n",
    "val_term_doc = vec.transform(val['ABSTRACT'])\n",
    "test_term_doc = vec.transform(test_data['ABSTRACT'])\n",
    "\n",
    "\n",
    "y_pred_prob = np.zeros((val_term_doc.shape[0], len(TAGS)))\n",
    "\n",
    "models = []\n",
    "log_rats = []\n",
    "\n",
    "for i, tag in enumerate(TAGS):\n",
    "    print('Fitting ..', tag)\n",
    "    model,log_rat = get_model(trn_term_doc,train[tag])\n",
    "    models.append(model)\n",
    "    log_rats.append(log_rat)\n",
    "    y_pred_prob[:,i] = model.predict_proba(val_term_doc.multiply(log_rat))[:,1]\n",
    "\n",
    "\n",
    "best_thresholds = get_cut_offthreshold(y_pred_prob,val,TAGS)\n",
    "y_pred = get_predictions(y_pred_prob,best_thresholds,TAGS)\n",
    "\n",
    "print(\"F1 Score on Validation Set\", f1_score(val[TAGS], y_pred, average='micro'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = np.zeros((test_term_doc.shape[0], len(TAGS)))\n",
    "\n",
    "for i, tag in enumerate(TAGS):   \n",
    "    y_pred_prob[:,i] = models[i].predict_proba(test_term_doc.multiply(log_rats[i]))[:,1]\n",
    "\n",
    "\n",
    "    \n",
    "predictions = get_predictions(y_pred_prob,best_thresholds,TAGS)\n",
    "\n",
    "result = pd.DataFrame(predictions)\n",
    "result.columns = TAGS\n",
    "result['id'] = test_data['id']\n",
    "\n",
    "result.to_csv(\"nbsvm.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5 : NB-SVM using TFIDF Features with Topic Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting .. Analysis of PDEs\n",
      "Fitting .. Applications\n",
      "Fitting .. Artificial Intelligence\n",
      "Fitting .. Astrophysics of Galaxies\n",
      "Fitting .. Computation and Language\n",
      "Fitting .. Computer Vision and Pattern Recognition\n",
      "Fitting .. Cosmology and Nongalactic Astrophysics\n",
      "Fitting .. Data Structures and Algorithms\n",
      "Fitting .. Differential Geometry\n",
      "Fitting .. Earth and Planetary Astrophysics\n",
      "Fitting .. Fluid Dynamics\n",
      "Fitting .. Information Theory\n",
      "Fitting .. Instrumentation and Methods for Astrophysics\n",
      "Fitting .. Machine Learning\n",
      "Fitting .. Materials Science\n",
      "Fitting .. Methodology\n",
      "Fitting .. Number Theory\n",
      "Fitting .. Optimization and Control\n",
      "Fitting .. Representation Theory\n",
      "Fitting .. Robotics\n",
      "Fitting .. Social and Information Networks\n",
      "Fitting .. Statistics Theory\n",
      "Fitting .. Strongly Correlated Electrons\n",
      "Fitting .. Superconductivity\n",
      "Fitting .. Systems and Control\n",
      "F1 Score on Validation Set 0.7521122726621797\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def get_val_feats(x_feats,log_rat,topic_cols):\n",
    "    x_feats_nb = x_feats.multiply(log_rat)    \n",
    "    x_feats_nb_topic_cols = np.hstack((x_feats_nb.toarray(),topic_cols))\n",
    "    return x_feats_nb_topic_cols\n",
    "\n",
    "vec = TfidfVectorizer(ngram_range=(1,2), min_df=3, max_df=0.9,use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1,max_features=10000)\n",
    "\n",
    "trn_term_doc = vec.fit_transform(train['ABSTRACT'])\n",
    "val_term_doc = vec.transform(val['ABSTRACT'])\n",
    "test_term_doc = vec.transform(test_data['ABSTRACT'])\n",
    "\n",
    "\n",
    "y_pred_prob = np.zeros((val_term_doc.shape[0], len(TAGS)))\n",
    "\n",
    "models = []\n",
    "log_rats = []\n",
    "\n",
    "for i, tag in enumerate(TAGS):\n",
    "    print('Fitting ..', tag)\n",
    "    model,log_rat = get_model_topic(trn_term_doc,train[TOPIC_COLS],train[tag])\n",
    "    models.append(model)\n",
    "    log_rats.append(log_rat)\n",
    "    y_pred_prob[:,i] = model.predict_proba(get_val_feats(val_term_doc,log_rat,val[TOPIC_COLS]))[:,1]\n",
    "\n",
    "\n",
    "best_thresholds = get_cut_offthreshold(y_pred_prob,val,TAGS)\n",
    "y_pred = get_predictions(y_pred_prob,best_thresholds,TAGS)\n",
    "\n",
    "print(\"F1 Score on Validation Set\", f1_score(val[TAGS], y_pred, average='micro'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = np.zeros((test_term_doc.shape[0], len(TAGS)))\n",
    "\n",
    "for i, tag in enumerate(TAGS):   \n",
    "    y_pred_prob[:,i] = models[i].predict_proba(get_val_feats(test_term_doc,log_rats[i],test_data[TOPIC_COLS]))[:,1]\n",
    "\n",
    "\n",
    "    \n",
    "predictions = get_predictions(y_pred_prob,best_thresholds,TAGS)\n",
    "\n",
    "result = pd.DataFrame(predictions)\n",
    "result.columns = TAGS\n",
    "result['id'] = test_data['id']\n",
    "\n",
    "result.to_csv(\"nbsvm_topic_cols.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
