{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data = pd.read_csv(\"data/Train.csv\")\n",
    "tags = pd.read_csv(\"data/Tags.csv\")\n",
    "test_data = pd.read_csv(\"data/Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14004 entries, 0 to 14003\n",
      "Data columns (total 31 columns):\n",
      " #   Column                                        Non-Null Count  Dtype \n",
      "---  ------                                        --------------  ----- \n",
      " 0   id                                            14004 non-null  int64 \n",
      " 1   ABSTRACT                                      14004 non-null  object\n",
      " 2   Computer Science                              14004 non-null  int64 \n",
      " 3   Mathematics                                   14004 non-null  int64 \n",
      " 4   Physics                                       14004 non-null  int64 \n",
      " 5   Statistics                                    14004 non-null  int64 \n",
      " 6   Analysis of PDEs                              14004 non-null  int64 \n",
      " 7   Applications                                  14004 non-null  int64 \n",
      " 8   Artificial Intelligence                       14004 non-null  int64 \n",
      " 9   Astrophysics of Galaxies                      14004 non-null  int64 \n",
      " 10  Computation and Language                      14004 non-null  int64 \n",
      " 11  Computer Vision and Pattern Recognition       14004 non-null  int64 \n",
      " 12  Cosmology and Nongalactic Astrophysics        14004 non-null  int64 \n",
      " 13  Data Structures and Algorithms                14004 non-null  int64 \n",
      " 14  Differential Geometry                         14004 non-null  int64 \n",
      " 15  Earth and Planetary Astrophysics              14004 non-null  int64 \n",
      " 16  Fluid Dynamics                                14004 non-null  int64 \n",
      " 17  Information Theory                            14004 non-null  int64 \n",
      " 18  Instrumentation and Methods for Astrophysics  14004 non-null  int64 \n",
      " 19  Machine Learning                              14004 non-null  int64 \n",
      " 20  Materials Science                             14004 non-null  int64 \n",
      " 21  Methodology                                   14004 non-null  int64 \n",
      " 22  Number Theory                                 14004 non-null  int64 \n",
      " 23  Optimization and Control                      14004 non-null  int64 \n",
      " 24  Representation Theory                         14004 non-null  int64 \n",
      " 25  Robotics                                      14004 non-null  int64 \n",
      " 26  Social and Information Networks               14004 non-null  int64 \n",
      " 27  Statistics Theory                             14004 non-null  int64 \n",
      " 28  Strongly Correlated Electrons                 14004 non-null  int64 \n",
      " 29  Superconductivity                             14004 non-null  int64 \n",
      " 30  Systems and Control                           14004 non-null  int64 \n",
      "dtypes: int64(30), object(1)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_COLS = ['Computer Science','Mathematics','Physics','Statistics']\n",
    "TAGS = list(tags['Tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['ABSTRACT'] = train_data['ABSTRACT'].str.lower()\n",
    "test_data['ABSTRACT'] = test_data['ABSTRACT'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "train_data['ABSTRACT'] = train_data['ABSTRACT'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "test_data['ABSTRACT'] = test_data['ABSTRACT'].str.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, val = train_test_split(train_data,test_size=0.2,random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best threshold for each label\n",
    "def get_cut_offthreshold(y_pred_prob,validation_set,TAGS):\n",
    "    thresholds = np.array(list(range(0,100)))/100.0\n",
    "    best_thresholds = []\n",
    "    for idx in range(0,25):\n",
    "        scores = [f1_score(validation_set[TAGS[idx]], y_pred_prob[:,idx] > thresh, average='micro') for thresh in thresholds]\n",
    "        best_thresh = thresholds[np.argmax(scores)]\n",
    "        best_thresholds.append(best_thresh)\n",
    "    return best_thresholds\n",
    "\n",
    "# Get predictions based on probabilities and class specific thresholds\n",
    "def get_predictions(pred_prob,best_thresholds,TAGS):\n",
    "    predictions = np.zeros((pred_prob.shape[0],len(TAGS)))\n",
    "    for idx in range(0,25):\n",
    "        predictions[:,idx] = pred_prob[:,idx] > best_thresholds[idx]    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Averaged Glove Word Embeddings with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "glove_input_file = 'glove/glove.6B/glove.6B.100d.txt'\n",
    "word2vec_output_file = 'glove/glove.6B/glove.6B.100d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Stanford GloVe model\n",
    "filename = 'glove/glove.6B/glove.6B.100d.txt.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Averaged Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_vec = np.zeros(shape=(100,))\n",
    "\n",
    "def get_sent_emb(words,w2v_vocabulary):\n",
    "    word_emb = np.array([])\n",
    "    for each in words.split():\n",
    "        if each in w2v_vocabulary:\n",
    "            emb = model[each]\n",
    "#         else:\n",
    "#             emb = zero_vec\n",
    "            if word_emb.shape[0] == 0:\n",
    "                word_emb = np.expand_dims(emb,axis=0)\n",
    "            else:\n",
    "                word_emb = np.concatenate((word_emb,np.expand_dims(emb,axis=0)),axis=0)\n",
    "    word_emb = np.array(word_emb)\n",
    "    return np.expand_dims(np.mean(word_emb,axis=0),axis=0)\n",
    "\n",
    "w2v_vocabulary = model.vocab\n",
    "sentence_emb = np.array([])\n",
    "for each in list(train['ABSTRACT']) + list(val['ABSTRACT']) + list(test_data['ABSTRACT']):\n",
    "    if sentence_emb.shape[0] == 0:\n",
    "        sentence_emb = get_sent_emb(each,w2v_vocabulary)\n",
    "    else:\n",
    "        sentence_emb = np.concatenate((sentence_emb,get_sent_emb(each,w2v_vocabulary)),axis=0)\n",
    "\n",
    "sentence_emb = np.array(sentence_emb)\n",
    "\n",
    "# sentence_emb = normalize(sentence_emb,axis=1)\n",
    "train_emb = sentence_emb[:len(train)]\n",
    "val_emb = sentence_emb[len(train):len(train)+len(val)]\n",
    "test_emb = sentence_emb[len(train)+len(val):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=10, max_iter=1000, n_jobs=1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "\n",
    "clf = OneVsRestClassifier(LogisticRegression(C=10,n_jobs=1,max_iter=1000))\n",
    "clf.fit(train_emb,train[TAGS])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on Validation Set 0.5534709193245778\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = clf.predict_proba(val_emb)\n",
    "best_thresholds = get_cut_offthreshold(y_pred_prob,val,TAGS)\n",
    "y_pred = get_predictions(y_pred_prob,best_thresholds,TAGS)\n",
    "\n",
    "print(\"F1 Score on Validation Set\", f1_score(val[TAGS], y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on Test\n",
    "\n",
    "y_pred_test_prob = clf.predict_proba(test_emb)\n",
    "\n",
    "predictions = get_predictions(y_pred_test_prob,best_thresholds,TAGS)\n",
    "\n",
    "result = pd.DataFrame(predictions)\n",
    "result.columns = TAGS\n",
    "result['id'] = test_data['id']\n",
    "result.to_csv(\"glove_word_logistic.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
